{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask-RCNN inference with tensorflow, onnxruntime, TensorRT engine.  Balloon dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%cd /src/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from layers import losses\n",
    "from training import get_optimizer\n",
    "from model import mask_rcnn_functional\n",
    "from common import inference_utils\n",
    "from common.inference_utils import process_input\n",
    "from common import utils\n",
    "from common.config import CONFIG\n",
    "\n",
    "import tensorflow as tf\n",
    "utils.tf_limit_gpu_memory(tf, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = '/src/result_models/resnet18_256x256_500/maskrcnn_resnet18_16a5e7ed4b511704027fb29c476f9928_cp-0012.ckpt'\n",
    "# weights_path = '/src/result_models/256x256/maskrcnn_mobilenet_1e3046627e7e8bc073e8b9e50b354411_cp-0002.ckpt'\n",
    "# weights_path = '/src/result_models/512x512/maskrcnn_mobilenet_c1f61e61570ae80cd3c574c008cbf226_cp-0010.ckpt'\n",
    "weights_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading inference graph and import weights\n",
    "from samples.plates import plates\n",
    "\n",
    "CONFIG.update(plates.COCO_CONFIG)\n",
    "\n",
    "CONFIG.update({\n",
    "                'image_shape': (256, 256, 3),\n",
    "                'backbone': 'resnet18',\n",
    "                'image_resize_mode': 'square',\n",
    "                'img_size': 256,\n",
    "#                 'image_min_dim': 200,\n",
    "                'image_min_scale': 0,\n",
    "                'image_max_dim': 256,\n",
    "                'batch_size': 1,\n",
    "                'images_per_gpu': 1,\n",
    "\n",
    "                }\n",
    "    )\n",
    "\n",
    "inference_config = CONFIG\n",
    "inference_config.update({'training': False})\n",
    "inference_model = mask_rcnn_functional(config=inference_config)\n",
    "inference_model = inference_utils.load_mrcnn_weights(model=inference_model,\n",
    "                                                     weights_path=weights_path,\n",
    "                                                     verbose=True\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Run several tests with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/cx-ir/patentes_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import augmentation as aug\n",
    "from preprocess import preprocess\n",
    "\n",
    "\n",
    "base_dir = r'/data/cx-ir/patentes_500'\n",
    "train_dir = base_dir\n",
    "val_dir = base_dir\n",
    "eval_dataset = plates.PlateDataset(dataset_dir=base_dir,\n",
    "                               subset='test',\n",
    "                               # SegmentationDataset necessary parent attributes\n",
    "                               augmentation=aug.get_validation_augmentation(\n",
    "                                           image_size=CONFIG['img_size'],\n",
    "                                           normalize=CONFIG['normalization']\n",
    "                               ),\n",
    "                               **CONFIG\n",
    "                              )\n",
    "eval_dataloader = preprocess.DataLoader(eval_dataset,\n",
    "                                        shuffle=True,\n",
    "                                        cast_output=False,\n",
    "                                        return_original=True,\n",
    "                                         **CONFIG\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0 padded imagen\n",
    "1 padded mask\n",
    "2 labels ?\n",
    "3 boxes\n",
    "4 masks? RLÂ·\n",
    "5 original image\n",
    "6 original mask\n",
    "7 label ?\n",
    "8 boxes original probablemente\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "for data in eval_dataset:\n",
    "    \n",
    "    img = data[5]\n",
    "    img_processed, image_meta, window = process_input(img, CONFIG)\n",
    "    \n",
    "    output = inference_model([np.expand_dims(img_processed, 0),\n",
    "                              np.expand_dims(image_meta, 0)]\n",
    "                            ) \n",
    "    \n",
    "    detections, mrcnn_probs, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox = output\n",
    "   \n",
    "    \n",
    "    # Extract bboxes, class_ids, scores and full-size masks\n",
    "    boxes, class_ids, scores, full_masks = \\\n",
    "    utils.reformat_detections(detections=detections[0].numpy(), \n",
    "                              mrcnn_mask=mrcnn_mask[0].numpy(), \n",
    "                              original_image_shape=img.shape, \n",
    "                              image_shape=img_processed.shape, \n",
    "                              window=window\n",
    "                             )\n",
    "    \n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    plt.title('Input data')\n",
    "\n",
    "    plt.imshow(img, 'gray', interpolation='none')\n",
    "    out_data = zip(class_ids, scores,boxes, np.moveaxis(full_masks, -1, 0))\n",
    "    out_data = sorted(filter(lambda x: x[1]>=.9,out_data), key=lambda y: y[1], reverse=True)\n",
    "    if any(out_data):\n",
    "        c, s, box, fm = out_data[0]\n",
    "        print\n",
    "        plt.imshow(fm, 'jet', interpolation='none', alpha=0.3)\n",
    "        plt.title(f'Mask. class_id: {c} score: {s}')\n",
    "    # for c, s, fm in zip(class_ids, scores, np.moveaxis(full_masks, -1, 0)):\n",
    "\n",
    "    #     fig=plt.figure(figsize=(5,5))\n",
    "    #     plt.title(f'Mask. class_id: {c} score: {s}')\n",
    "    #     plt.imshow(fm)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert model to .onnx with tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "# import onnx_graphsurgeon as gs\n",
    "from common.inference_optimize import maskrcnn_to_onnx, modify_onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_spec = (\n",
    "    tf.TensorSpec((CONFIG['batch_size'], *CONFIG['image_shape']), tf.float32, name=\"input_image\"),\n",
    "    tf.TensorSpec((CONFIG['batch_size'], CONFIG['meta_shape']), tf.float32, name=\"input_image_meta\")\n",
    ")\n",
    "base_folder = os.path.dirname(weights_path)\n",
    "output_path = os.path.join(base_folder,f\"\"\"maskrcnn_{CONFIG['backbone']}_{'_'.join(list(map(str, CONFIG['image_shape'])))}.onnx\"\"\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskrcnn_to_onnx(model=inference_model, \n",
    "                 output_path = output_path,\n",
    "                 input_spec=input_spec,\n",
    "                 kwargs={'opset': 11}\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### Load onnx model and check it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "model = onnx.load(output_path)\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### Run several tests with onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = ort.InferenceSession(output_path)\n",
    "print(f'Inputs: {[x.name for x in sess.get_inputs()]}\\nOutputs:{[x.name for x in sess.get_outputs()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img_name in os.listdir(test_images_path):\n",
    "for data in eval_dataset:\n",
    "    \n",
    "    img = data[5]\n",
    "#     img = cv2.imread(os.path.join(test_images_path, img_name))\n",
    "    img_processed, image_meta, window = process_input(img, CONFIG)\n",
    "    \n",
    "\n",
    "    output = sess.run(output_names=[x.name for x in sess.get_outputs()], \n",
    "                      input_feed={'input_image': np.expand_dims(img_processed, 0).astype('float32'),\n",
    "                                  'input_image_meta': np.expand_dims(image_meta, 0).astype('float32'),\n",
    "                                 }\n",
    "                     )\n",
    "    \n",
    "    detections, mrcnn_probs, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox = output\n",
    "    \n",
    "#     print(img_name, '\\nOutput shapes:')\n",
    "#     for out in output:\n",
    "#         print(out.shape)\n",
    "    \n",
    "    \n",
    "    # Extract bboxes, class_ids, scores and full-size masks\n",
    "    boxes, class_ids, scores, full_masks = \\\n",
    "    utils.reformat_detections(detections=detections[0], \n",
    "                              mrcnn_mask=mrcnn_mask[0], \n",
    "                              original_image_shape=img.shape, \n",
    "                              image_shape=img_processed.shape, \n",
    "                              window=window\n",
    "                             )\n",
    "    \n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    plt.title('Input data')\n",
    "\n",
    "    plt.imshow(img, 'gray', interpolation='none')\n",
    "    out_data = zip(class_ids, scores,boxes, np.moveaxis(full_masks, -1, 0))\n",
    "    out_data = sorted(filter(lambda x: x[1]>=.9,out_data), key=lambda y: y[1], reverse=True)\n",
    "    if any(out_data):\n",
    "        c, s, box, fm = out_data[0]\n",
    "        print\n",
    "        plt.imshow(fm, 'jet', interpolation='none', alpha=0.3)\n",
    "        plt.title(f'Mask. class_id: {c} score: {s}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### Configure model for TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_onnx_model(model_path=output_path,\n",
    "                  config=CONFIG,\n",
    "                  verbose=True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### TensorRT optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/src/result_models/resnet18_256x256_500/maskrcnn_resnet18_256_256_3_trt_mod.onnx'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_mod_path = output_path.replace('.onnx', '_trt_mod.onnx')\n",
    "trt_path_32 = onnx_mod_path.replace('.onnx', '_fp32.engine')\n",
    "trt_path_16 = onnx_mod_path.replace('.onnx', '_fp16.engine')\n",
    "trt_path\n",
    "onnx_mod_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/src/result_models/resnet18_256x256_500/maskrcnn_resnet18_256_256_3_trt_mod_fp32.engine'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt_path_32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__With trtexec:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "os.chdir('../weights')\n",
    "\n",
    "# Construct appropriate command\n",
    "fp16_mode = False\n",
    "command = [os.environ['TRTEXEC'],\n",
    "           f'--onnx={onnx_mod_path}',\n",
    "           f'--saveEngine={trt_path}',\n",
    "            '--workspace=2048',\n",
    "            '--explicitBatch',\n",
    "            '--verbose',\n",
    "          ]\n",
    "\n",
    "# fp16 param\n",
    "if fp16_mode:\n",
    "    command[2].replace('32', '16')\n",
    "    command.append('--fp16')\n",
    "\n",
    "# tacticSources param\n",
    "# Do not neeed on jetson with aarch64 architecture for now.\n",
    "arch = os.uname().machine\n",
    "if arch == 'x86_64':\n",
    "    command.append('--tacticSources=-cublasLt,+cublas')\n",
    "    \n",
    "print(f'\\nArch: {arch}\\ntrtexec command list: {command}')\n",
    "\n",
    "result = subprocess.run(command, capture_output=True, check=True)\n",
    "# Print stdout inference result\n",
    "print(result.stdout.decode('utf8')[-2495:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__With python TensorRT API:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size = 1\n",
    "# Precision mode\n",
    "fp16_mode = False\n",
    "# Workspace size in Mb\n",
    "wspace_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of detected layers:  235\n",
      "Detected inputs:  2\n",
      "Detected outputs:  2\n",
      "CPU times: user 24.1 s, sys: 2.53 s, total: 26.6 s\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Init TensorRT Logger\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)\n",
    "# Init TensorRT plugins\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")\n",
    "# Set tensorrt-prepared onnx model\n",
    "onnx_model_path = onnx_mod_path\n",
    "# Use explicit batch\n",
    "explicit_batch = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "        builder.create_builder_config() as builder_config, \\\n",
    "        builder.create_network(explicit_batch) as network, \\\n",
    "        trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "    with open(onnx_model_path, 'rb') as model:\n",
    "        parser.parse(model.read())\n",
    "\n",
    "    print('Num of detected layers: ', network.num_layers)\n",
    "    print('Detected inputs: ', network.num_inputs)\n",
    "    print('Detected outputs: ', network.num_outputs)\n",
    "    \n",
    "    # Workspace size\n",
    "    # 1e6 bytes == 1Mb\n",
    "    builder_config.max_workspace_size = int(1e6 * wspace_size)\n",
    "    \n",
    "    # Precision mode\n",
    "    if fp16_mode:\n",
    "        builder_config.set_flag(trt.BuilderFlag.FP16)\n",
    "    \n",
    "    # Max batch size\n",
    "    builder.max_batch_size = max_batch_size\n",
    "    \n",
    "    # Set the list of tactic sources\n",
    "    # Do not need for Jetson with aarch64 architecture for now\n",
    "    arch = os.uname().machine\n",
    "    if arch == 'x86_64':\n",
    "        tactic_source = 1 << int(trt.TacticSource.CUBLAS) | 0 << int(trt.TacticSource.CUBLAS_LT)\n",
    "        builder_config.set_tactic_sources(tactic_source)\n",
    "        \n",
    "    \n",
    "    # Make TensorRT engine\n",
    "    engine = builder.build_engine(network, builder_config)\n",
    "    \n",
    "    # Save TensorRT engine\n",
    "    if fp16_mode:\n",
    "        trt_model_name = trt_path_16\n",
    "    else:\n",
    "        trt_model_name = trt_path_32\n",
    "\n",
    "    with open(trt_model_name, \"wb\") as f:\n",
    "        f.write(engine.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### Run TensorRT inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trt_mrcnn_inference(model, image):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        model: tensorflow tf.keras.Model\n",
    "        image: prepared image for inference\n",
    "\n",
    "    Returns: boxes,\n",
    "             class_ids, \n",
    "             scores, f\n",
    "             ull_masks, \n",
    "             eval_gt_boxes, \n",
    "             eval_gt_class_ids, \n",
    "             eval_gt_masks\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract trt-variables from a dict for transparency\n",
    "    engine = model['engine']\n",
    "    stream = model['stream']\n",
    "    context = model['context']\n",
    "    device_input = model['device_input']\n",
    "    device_output1 = model['device_output1']\n",
    "    device_output2 = model['device_output2']\n",
    "\n",
    "    host_output1 = model['host_output1']\n",
    "    host_output2 = model['host_output2']\n",
    "\n",
    "    # Make inference\n",
    "    host_input = image.astype(dtype=np.float32, order='C')\n",
    "    cuda.memcpy_htod_async(device_input, host_input, stream)\n",
    "    context.execute_async(bindings=[int(device_input),\n",
    "                                    int(device_output1),\n",
    "                                    int(device_output2),\n",
    "                                    ],\n",
    "                          stream_handle=stream.handle)\n",
    "\n",
    "    cuda.memcpy_dtoh_async(host_output1, device_output1, stream)\n",
    "    cuda.memcpy_dtoh_async(host_output2, device_output2, stream)\n",
    "    stream.synchronize()\n",
    "    \n",
    "    trt_mrcnn_detection = host_output1.reshape(\n",
    "        engine.get_binding_shape('mrcnn_detection')).astype(dtype=np.float32)\n",
    "    trt_mrcnn_mask = host_output2.reshape(\n",
    "        engine.get_binding_shape('mrcnn_mask')).astype(dtype=np.float32)\n",
    "    \n",
    "    return trt_mrcnn_detection, trt_mrcnn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mrcnn_trt_engine(model_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Load TensorRT engine via pycuda\n",
    "    Args:\n",
    "        model_path: model path to TensorRT-engine\n",
    "\n",
    "    Returns: python dict of attributes for pycuda model inference\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    trt_logger = trt.Logger(trt.Logger.VERBOSE)\n",
    "    trt.init_libnvinfer_plugins(trt_logger, \"\")\n",
    "\n",
    "    with open(model_path, \"rb\") as f, trt.Runtime(trt_logger) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Inputs\n",
    "    input_shape = engine.get_binding_shape('input_image')\n",
    "    input_size = trt.volume(input_shape) *\\\n",
    "                 engine.max_batch_size * np.dtype(np.float32).itemsize\n",
    "    device_input = cuda.mem_alloc(input_size)\n",
    "\n",
    "    # Outputs\n",
    "    output_names = list(engine)[1:]\n",
    "\n",
    "    # mrcnn_detection output\n",
    "    output_shape1 = engine.get_binding_shape('mrcnn_detection')\n",
    "    host_output1 = cuda.pagelocked_empty(trt.volume(output_shape1) *\n",
    "                                              engine.max_batch_size,\n",
    "                                              dtype=np.float32)\n",
    "    device_output1 = cuda.mem_alloc(host_output1.nbytes)\n",
    "\n",
    "\n",
    "    # mrcnn_mask output\n",
    "    output_shape2 = engine.get_binding_shape('mrcnn_mask')\n",
    "    host_output2 = cuda.pagelocked_empty(trt.volume(output_shape2) * engine.max_batch_size,\n",
    "                                              dtype=np.float32)\n",
    "    device_output2 = cuda.mem_alloc(host_output2.nbytes)\n",
    "\n",
    "    # Setting a cuda stream\n",
    "    stream = cuda.Stream()\n",
    "    \n",
    "    return {'engine': engine,\n",
    "            'stream': stream,\n",
    "            'context': context,\n",
    "            'device_input': device_input,\n",
    "            'device_output1': device_output1,\n",
    "            'device_output2':device_output2,\n",
    "            'host_output1': host_output1,\n",
    "            'host_output2': host_output2\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model = set_mrcnn_trt_engine(f\"\"\"../weights/maskrcnn_{CONFIG['backbone']}_512_512_3_trt_mod_fp32.engine\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name in os.listdir(test_images_path):\n",
    "    img = cv2.imread(os.path.join(test_images_path, img_name))\n",
    "    img_processed, image_meta, window = process_input(img, CONFIG)\n",
    "    \n",
    "\n",
    "    trt_mrcnn_detection, trt_mrcnn_mask = trt_mrcnn_inference(trt_model, np.expand_dims(img_processed, 0))\n",
    "    \n",
    "\n",
    "    # Extract bboxes, class_ids, scores and full-size masks\n",
    "    boxes, class_ids, scores, full_masks = \\\n",
    "    utils.reformat_detections(detections=trt_mrcnn_detection[0], \n",
    "                              mrcnn_mask=trt_mrcnn_mask[0], \n",
    "                              original_image_shape=img.shape, \n",
    "                              image_shape=img_processed.shape, \n",
    "                              window=window\n",
    "                             )\n",
    "    \n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    plt.title('Input data')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    for c, s, fm in zip(class_ids, scores, np.moveaxis(full_masks, -1, 0)):\n",
    "\n",
    "        fig=plt.figure(figsize=(5,5))\n",
    "        plt.title(f'Mask. class_id: {c} score: {s}')\n",
    "        plt.imshow(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
