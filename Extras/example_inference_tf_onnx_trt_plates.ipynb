{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Mask-RCNN inference with tensorflow, onnxruntime, TensorRT engine.  Balloon dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "os.chdir('..')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import subprocess\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from layers import losses\n",
    "from training import get_optimizer\n",
    "from model import mask_rcnn_functional\n",
    "from common import inference_utils\n",
    "from common.inference_utils import process_input\n",
    "from common import utils\n",
    "from common.config import CONFIG\n",
    "\n",
    "import tensorflow as tf\n",
    "utils.tf_limit_gpu_memory(tf, 1500)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs Memory limit: 1500\n",
      "Physical GPU-devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare model for inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# weights_path= r\"D:\\Source\\maskrcnn_tf2\\src\\result_models\\512x512\\maskrcnn_mobilenet_45e162bb7fbae8f8dfbd03ee1daeb9b3_cp-0012.ckpt\"\n",
    "# weights_path = r'D:\\Source\\maskrcnn_tf2\\src\\result_models\\256x256\\maskrcnn_mobilenet_c7d9053b6f41759350d0161adec43865_cp-0002.ckpt'\n",
    "weights_path = r'models/maskrcnn_mobilenet_c1f61e61570ae80cd3c574c008cbf226_cp-0010.ckpt'\n",
    "weights_path"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'models/maskrcnn_mobilenet_c1f61e61570ae80cd3c574c008cbf226_cp-0010.ckpt'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Loading inference graph and import weights\n",
    "from samples.plates import plates\n",
    "# CONFIG.update(plates.COCO_CONFIG)\n",
    "\n",
    "CONFIG.update(plates.COCO_CONFIG)\n",
    "\n",
    "# CONFIG.update({\n",
    "#                 'image_shape': (256, 256, 3),\n",
    "#                 'image_resize_mode': 'square',\n",
    "#                 'img_size': 256,\n",
    "#                 'image_min_dim': 200,\n",
    "#                 'image_min_scale': 0,\n",
    "#                 'image_max_dim': 256,\n",
    "#                 'batch_size': 1,\n",
    "#                 'images_per_gpu': 1,\n",
    "\n",
    "#                 }\n",
    "#     )\n",
    "\n",
    "inference_config = CONFIG\n",
    "inference_config.update({'training': False})\n",
    "inference_model = mask_rcnn_functional(config=inference_config)\n",
    "inference_model = inference_utils.load_mrcnn_weights(model=inference_model,\n",
    "                                                     weights_path=weights_path,\n",
    "                                                     verbose=True\n",
    "                                                    )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[MaskRCNN] Inference mode\n",
      "WARNING:tensorflow:Layer norm_boxes_anchors is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[MaskRCNN] Backbone architecture: mobilenet\n",
      "\n",
      "Weights for inference graph will be transferred from training graph\n",
      "\n",
      "[MaskRCNN] Training mode\n",
      "WARNING:tensorflow:Layer norm_boxes_anchors is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[MaskRCNN] Backbone architecture: mobilenet\n",
      "MaskRCNN Losses:\n",
      "rpn_class_loss: <layers.losses.RPNClassLoss object at 0x7e72b09780>\n",
      "rpn_bbox_loss: <layers.losses.RPNBboxLoss object at 0x7e72b096a0>\n",
      "mrcnn_class_loss: <layers.losses.MRCNNClassLoss object at 0x7e72b095f8>\n",
      "mrcnn_bbox_loss: <layers.losses.MRCNNBboxLoss object at 0x7e72b09630>\n",
      "mrcnn_mask_loss: <layers.losses.MRCNNMaskLoss object at 0x7e72b09358>\n",
      "l2_regularizer: <layers.losses.L2RegLoss object at 0x7e72b09518>\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on models/maskrcnn_mobilenet_c1f61e61570ae80cd3c574c008cbf226_cp-0010.ckpt: Not found: models; No such file or directory",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-de3fb40c87d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m inference_model = inference_utils.load_mrcnn_weights(model=inference_model,\n\u001b[1;32m     24\u001b[0m                                                      \u001b[0mweights_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                                                     )\n",
      "\u001b[0;32m/src/src/common/inference_utils.py\u001b[0m in \u001b[0;36mload_mrcnn_weights\u001b[0;34m(model, weights_path, verbose)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m# Load training model checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;31m# Transfer weights from training to inference graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         model = weights_transfer(training_graph=training_model,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2176\u001b[0;31m         \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2177\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2178\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     93\u001b[0m   \"\"\"\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on models/maskrcnn_mobilenet_c1f61e61570ae80cd3c574c008cbf226_cp-0010.ckpt: Not found: models; No such file or directory"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "#### Run several tests with tensorflow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "test_images_path = r'/src/ttest'\n",
    "os.listdir(test_images_path)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/src/ttest'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e0e04021cebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_images_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'/src/ttest'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/src/ttest'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# %%time\n",
    "for img_name in os.listdir(test_images_path)[:2]:\n",
    "    img = cv2.imread(os.path.join(test_images_path, img_name))\n",
    "    img_processed, image_meta, window = process_input(img, CONFIG)\n",
    "    \n",
    "    output = inference_model([np.expand_dims(img_processed, 0),\n",
    "                              np.expand_dims(image_meta, 0)]\n",
    "                            ) \n",
    "    \n",
    "    detections, mrcnn_probs, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox = output\n",
    "    \n",
    "    # print(img_name, '\\nOutput shapes:')\n",
    "    # for out in output:\n",
    "    #     print(out.shape)\n",
    "    \n",
    "    \n",
    "    # Extract bboxes, class_ids, scores and full-size masks\n",
    "    boxes, class_ids, scores, full_masks = \\\n",
    "    utils.reformat_detections(detections=detections[0].numpy(), \n",
    "                              mrcnn_mask=mrcnn_mask[0].numpy(), \n",
    "                              original_image_shape=img.shape, \n",
    "                              image_shape=img_processed.shape, \n",
    "                              window=window\n",
    "                             )\n",
    "    \n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    plt.title('Input data')\n",
    "\n",
    "    plt.imshow(img, 'gray', interpolation='none')\n",
    "    out_data = zip(class_ids, scores, np.moveaxis(full_masks, -1, 0))\n",
    "    out_data = sorted(filter(lambda x: x[1]>=.9,out_data), key=lambda y: y[1], reverse=True)\n",
    "    # data= next(out_data)\n",
    "    # if data is None or not any(data):\n",
    "    #     continue\n",
    "    c, s, fm = out_data[0]\n",
    "\n",
    "    plt.imshow(fm, 'jet', interpolation='none', alpha=0.3)\n",
    "    plt.title(f'Mask. class_id: {c} score: {s}')\n",
    "    plt.show()    \n",
    "    # for c, s, fm in zip(class_ids, scores, np.moveaxis(full_masks, -1, 0)):\n",
    "\n",
    "    #     fig=plt.figure(figsize=(5,5))\n",
    "    #     plt.title(f'Mask. class_id: {c} score: {s}')\n",
    "    #     plt.imshow(fm)\n",
    "    plt.show()    "
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/src/ttest'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-66e8477c7eb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/src/ttest'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convert model to .onnx with tf2onnx"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "# import onnx_graphsurgeon as gs\n",
    "from common.inference_optimize import maskrcnn_to_onnx, modify_onnx_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_spec = (\n",
    "    tf.TensorSpec((CONFIG['batch_size'], *CONFIG['image_shape']), tf.float32, name=\"input_image\"),\n",
    "    tf.TensorSpec((CONFIG['batch_size'], CONFIG['meta_shape']), tf.float32, name=\"input_image_meta\")\n",
    ")\n",
    "model_name = f\"\"\"maskrcnn_{CONFIG['backbone']}_{'_'.join(list(map(str, CONFIG['image_shape'])))}\"\"\" "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "maskrcnn_to_onnx(model=inference_model, \n",
    "                 model_name = model_name,\n",
    "                 input_spec=input_spec,\n",
    "                 kwargs={'opset': 11}\n",
    "                )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load onnx model and check it "
   ],
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the ONNX model\n",
    "model = onnx.load(f\"\"\"../weights/maskrcnn_{CONFIG['backbone']}_512_512_3.onnx\"\"\")\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run several tests with onnxruntime"
   ],
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sess = ort.InferenceSession(f\"\"\"../weights/maskrcnn_{CONFIG['backbone']}_512_512_3.onnx\"\"\")\n",
    "print(f'Inputs: {[x.name for x in sess.get_inputs()]}\\nOutputs:{[x.name for x in sess.get_outputs()]}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for img_name in os.listdir(test_images_path):\n",
    "    img = cv2.imread(os.path.join(test_images_path, img_name))\n",
    "    img_processed, image_meta, window = process_input(img, CONFIG)\n",
    "    \n",
    "\n",
    "    output = sess.run(output_names=[x.name for x in sess.get_outputs()], \n",
    "                      input_feed={'input_image': np.expand_dims(img_processed, 0).astype('float32'),\n",
    "                                  'input_image_meta': np.expand_dims(image_meta, 0).astype('float32'),\n",
    "                                 }\n",
    "                     )\n",
    "    \n",
    "    detections, mrcnn_probs, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox = output\n",
    "    \n",
    "    print(img_name, '\\nOutput shapes:')\n",
    "    for out in output:\n",
    "        print(out.shape)\n",
    "    \n",
    "    \n",
    "    # Extract bboxes, class_ids, scores and full-size masks\n",
    "    boxes, class_ids, scores, full_masks = \\\n",
    "    utils.reformat_detections(detections=detections[0], \n",
    "                              mrcnn_mask=mrcnn_mask[0], \n",
    "                              original_image_shape=img.shape, \n",
    "                              image_shape=img_processed.shape, \n",
    "                              window=window\n",
    "                             )\n",
    "    \n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    plt.title('Input data')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    for c, s, fm in zip(class_ids, scores, np.moveaxis(full_masks, -1, 0)):\n",
    "\n",
    "        fig=plt.figure(figsize=(5,5))\n",
    "        plt.title(f'Mask. class_id: {c} score: {s}')\n",
    "        plt.imshow(fm)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Configure model for TensorRT"
   ],
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "modify_onnx_model(model_path=f'../weights/{model_name}.onnx',\n",
    "                  config=CONFIG,\n",
    "                  verbose=True\n",
    "                 )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TensorRT optimization\n",
    "\n",
    "__With trtexec:__ "
   ],
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "os.chdir('../weights')\n",
    "\n",
    "# Construct appropriate command\n",
    "fp16_mode = False\n",
    "command = [os.environ['TRTEXEC'],\n",
    "           f'--onnx={model_name}_trt_mod.onnx',\n",
    "           f'--saveEngine={model_name}_trt_mod_fp32.engine',\n",
    "            '--workspace=2048',\n",
    "            '--explicitBatch',\n",
    "            '--verbose',\n",
    "          ]\n",
    "\n",
    "# fp16 param\n",
    "if fp16_mode:\n",
    "    command[2].replace('32', '16')\n",
    "    command.append('--fp16')\n",
    "\n",
    "# tacticSources param\n",
    "# Do not neeed on jetson with aarch64 architecture for now.\n",
    "arch = os.uname().machine\n",
    "if arch == 'x86_64':\n",
    "    command.append('--tacticSources=-cublasLt,+cublas')\n",
    "    \n",
    "print(f'\\nArch: {arch}\\ntrtexec command list: {command}')\n",
    "\n",
    "result = subprocess.run(command, capture_output=True, check=True)\n",
    "# Print stdout inference result\n",
    "print(result.stdout.decode('utf8')[-2495:])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "__With python TensorRT API:__\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_batch_size = 1\n",
    "# Precision mode\n",
    "fp16_mode = False\n",
    "# Workspace size in Mb\n",
    "wspace_size = 1024"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "# Init TensorRT Logger\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)\n",
    "# Init TensorRT plugins\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")\n",
    "# Set tensorrt-prepared onnx model\n",
    "onnx_model_path = f'../weights/{model_name}_trt_mod.onnx'\n",
    "# Use explicit batch\n",
    "explicit_batch = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "        builder.create_builder_config() as builder_config, \\\n",
    "        builder.create_network(explicit_batch) as network, \\\n",
    "        trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "    with open(onnx_model_path, 'rb') as model:\n",
    "        parser.parse(model.read())\n",
    "\n",
    "    print('Num of detected layers: ', network.num_layers)\n",
    "    print('Detected inputs: ', network.num_inputs)\n",
    "    print('Detected outputs: ', network.num_outputs)\n",
    "    \n",
    "    # Workspace size\n",
    "    # 1e6 bytes == 1Mb\n",
    "    builder_config.max_workspace_size = int(1e6 * wspace_size)\n",
    "    \n",
    "    # Precision mode\n",
    "    if fp16_mode:\n",
    "        builder_config.set_flag(trt.BuilderFlag.FP16)\n",
    "    \n",
    "    # Max batch size\n",
    "    builder.max_batch_size = max_batch_size\n",
    "    \n",
    "    # Set the list of tactic sources\n",
    "    # Do not need for Jetson with aarch64 architecture for now\n",
    "    arch = os.uname().machine\n",
    "    if arch == 'x86_64':\n",
    "        tactic_source = 1 << int(trt.TacticSource.CUBLAS) | 0 << int(trt.TacticSource.CUBLAS_LT)\n",
    "        builder_config.set_tactic_sources(tactic_source)\n",
    "        \n",
    "    \n",
    "    # Make TensorRT engine\n",
    "    engine = builder.build_engine(network, builder_config)\n",
    "    \n",
    "    # Save TensorRT engine\n",
    "    if fp16_mode:\n",
    "        trt_model_name = f'../weights/{model_name}_fp16_trt.engine'\n",
    "    else:\n",
    "        trt_model_name = f'../weights/{model_name}_fp32_trt.engine'\n",
    "\n",
    "    with open(trt_model_name, \"wb\") as f:\n",
    "        f.write(engine.serialize())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run TensorRT inference"
   ],
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def trt_mrcnn_inference(model, image):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        model: tensorflow tf.keras.Model\n",
    "        image: prepared image for inference\n",
    "\n",
    "    Returns: boxes,\n",
    "             class_ids, \n",
    "             scores, f\n",
    "             ull_masks, \n",
    "             eval_gt_boxes, \n",
    "             eval_gt_class_ids, \n",
    "             eval_gt_masks\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract trt-variables from a dict for transparency\n",
    "    engine = model['engine']\n",
    "    stream = model['stream']\n",
    "    context = model['context']\n",
    "    device_input = model['device_input']\n",
    "    device_output1 = model['device_output1']\n",
    "    device_output2 = model['device_output2']\n",
    "\n",
    "    host_output1 = model['host_output1']\n",
    "    host_output2 = model['host_output2']\n",
    "\n",
    "    # Make inference\n",
    "    host_input = image.astype(dtype=np.float32, order='C')\n",
    "    cuda.memcpy_htod_async(device_input, host_input, stream)\n",
    "    context.execute_async(bindings=[int(device_input),\n",
    "                                    int(device_output1),\n",
    "                                    int(device_output2),\n",
    "                                    ],\n",
    "                          stream_handle=stream.handle)\n",
    "\n",
    "    cuda.memcpy_dtoh_async(host_output1, device_output1, stream)\n",
    "    cuda.memcpy_dtoh_async(host_output2, device_output2, stream)\n",
    "    stream.synchronize()\n",
    "    \n",
    "    trt_mrcnn_detection = host_output1.reshape(\n",
    "        engine.get_binding_shape('mrcnn_detection')).astype(dtype=np.float32)\n",
    "    trt_mrcnn_mask = host_output2.reshape(\n",
    "        engine.get_binding_shape('mrcnn_mask')).astype(dtype=np.float32)\n",
    "    \n",
    "    return trt_mrcnn_detection, trt_mrcnn_mask"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def set_mrcnn_trt_engine(model_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Load TensorRT engine via pycuda\n",
    "    Args:\n",
    "        model_path: model path to TensorRT-engine\n",
    "\n",
    "    Returns: python dict of attributes for pycuda model inference\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    trt_logger = trt.Logger(trt.Logger.VERBOSE)\n",
    "    trt.init_libnvinfer_plugins(trt_logger, \"\")\n",
    "\n",
    "    with open(model_path, \"rb\") as f, trt.Runtime(trt_logger) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Inputs\n",
    "    input_shape = engine.get_binding_shape('input_image')\n",
    "    input_size = trt.volume(input_shape) *\\\n",
    "                 engine.max_batch_size * np.dtype(np.float32).itemsize\n",
    "    device_input = cuda.mem_alloc(input_size)\n",
    "\n",
    "    # Outputs\n",
    "    output_names = list(engine)[1:]\n",
    "\n",
    "    # mrcnn_detection output\n",
    "    output_shape1 = engine.get_binding_shape('mrcnn_detection')\n",
    "    host_output1 = cuda.pagelocked_empty(trt.volume(output_shape1) *\n",
    "                                              engine.max_batch_size,\n",
    "                                              dtype=np.float32)\n",
    "    device_output1 = cuda.mem_alloc(host_output1.nbytes)\n",
    "\n",
    "\n",
    "    # mrcnn_mask output\n",
    "    output_shape2 = engine.get_binding_shape('mrcnn_mask')\n",
    "    host_output2 = cuda.pagelocked_empty(trt.volume(output_shape2) * engine.max_batch_size,\n",
    "                                              dtype=np.float32)\n",
    "    device_output2 = cuda.mem_alloc(host_output2.nbytes)\n",
    "\n",
    "    # Setting a cuda stream\n",
    "    stream = cuda.Stream()\n",
    "    \n",
    "    return {'engine': engine,\n",
    "            'stream': stream,\n",
    "            'context': context,\n",
    "            'device_input': device_input,\n",
    "            'device_output1': device_output1,\n",
    "            'device_output2':device_output2,\n",
    "            'host_output1': host_output1,\n",
    "            'host_output2': host_output2\n",
    "           }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trt_model = set_mrcnn_trt_engine(f\"\"\"../weights/maskrcnn_{CONFIG['backbone']}_512_512_3_trt_mod_fp32.engine\"\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for img_name in os.listdir(test_images_path):\n",
    "    img = cv2.imread(os.path.join(test_images_path, img_name))\n",
    "    img_processed, image_meta, window = process_input(img, CONFIG)\n",
    "    \n",
    "\n",
    "    trt_mrcnn_detection, trt_mrcnn_mask = trt_mrcnn_inference(trt_model, np.expand_dims(img_processed, 0))\n",
    "    \n",
    "\n",
    "    # Extract bboxes, class_ids, scores and full-size masks\n",
    "    boxes, class_ids, scores, full_masks = \\\n",
    "    utils.reformat_detections(detections=trt_mrcnn_detection[0], \n",
    "                              mrcnn_mask=trt_mrcnn_mask[0], \n",
    "                              original_image_shape=img.shape, \n",
    "                              image_shape=img_processed.shape, \n",
    "                              window=window\n",
    "                             )\n",
    "    \n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    plt.title('Input data')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    for c, s, fm in zip(class_ids, scores, np.moveaxis(full_masks, -1, 0)):\n",
    "\n",
    "        fig=plt.figure(figsize=(5,5))\n",
    "        plt.title(f'Mask. class_id: {c} score: {s}')\n",
    "        plt.imshow(fm)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}